{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment2_NiklasRindtorff.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "ek29zDAomkTO"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NiklasTR/bmi707/blob/master/Assignment2_NiklasRindtorff.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "hjgxJAFHU_LU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Assignment 2 - Convolutional Neural Networks (CNNs)"
      ]
    },
    {
      "metadata": {
        "id": "y30mEZgtVBtB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Your Name: Niklas Rindtorff\n",
        "\n",
        "Name of the Students You Worked With: I asked Ming for advice for the fine-tuning part, but implemented the code myself using the assignment 2 primer.\n",
        "\n",
        "References You Consulted: Lecture Excercises"
      ]
    },
    {
      "metadata": {
        "id": "YDST8U9VWoz2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Part 1: Convolutional Neural Networks for Chest X-Ray Images (80 points)\n",
        "## Data and Preliminaries\n",
        "In this assignment, we will explore the NIH CXR8 database (https://www.nih.gov/news-events/news-releases/nih-clinical-center-provides-one-largest-publicly-available-chest-x-ray-datasets-scientific-community). This chest X-ray database is the basis of a series highly-cited papers/preprint papers since 2017. You will examine the images in this dataset and build your own convolutional neural networks to classify the major diagnoses."
      ]
    },
    {
      "metadata": {
        "id": "JCZRIiEfVG0N",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Question 1: Data summary (10 points)\n",
        "Deep learning is not immune from the 'garbage in, garbage out' principle. Before digging into the data, it is recommended to get a sense of how the data was generated, understand the assumptions of the data, and review the data quality. We will ask you to answer some basic questions on the NIH CXR8 dataset. Please visit the website of the NIH CXR8 database, download the metadata (Data_Entry_2017.csv) and answer the following questions."
      ]
    },
    {
      "metadata": {
        "id": "gXtVh3IrVJgB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "####Question 1.1 (5 points)\n",
        "What is the file format of images in the NIH CXR8 database? What is the standard format for radiology image storage and transmission? How many images are there in the database? How many diagnostic categories are there in the database? What are they? How many images have more than one diagnosis?"
      ]
    },
    {
      "metadata": {
        "id": "rPUdn7NOVMJy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "__Your Answer:__\n",
        "\n",
        "The images are stored in the .png format. \n",
        "Usually, radiological images are stored in the DICOM format. This datatype is not available in the dataset.\n",
        "The database comprises 112,120 chest X-rays\n",
        "There are 14 disease categories. They are: \n",
        "\n",
        "* Atelectasis \n",
        "* Pneumothorax \n",
        "* Pneumonia \n",
        "* Pleural Thickening \n",
        "* Nodule \n",
        "* Mass \n",
        "* Infiltration \n",
        "* Hernia \n",
        "* Emphysema \n",
        "* Effusion \n",
        "* Edema \n",
        "* Consolidation \n",
        "* Cardiomegaly\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "4WTi3eKJb4So",
        "colab_type": "code",
        "outputId": "371d9f83-6fcc-4349-cb2e-349f73a02fa6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# I load the .csv file and identify the number of images with multiple diagnosis \n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "fF4LZ0aNdFaT",
        "colab_type": "code",
        "outputId": "5c05bf7b-a95d-41e0-c085-5c848095accd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "cell_type": "code",
      "source": [
        "# loading packages\n",
        "!ls gdrive/My\\ Drive/'Colab Notebooks'\n",
        "import pandas as pd"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Assignment1_Niklas_Rindtorff.ipynb\n",
            " Assignment2_NiklasRindtorff.ipynb\n",
            "'Copy of Assignment1_Niklas_Rindtorff.ipynb'\n",
            "'Copy of deep_learning_regression.ipynb'\n",
            "'Copy of Hello, Colaboratory'\n",
            "'Copy of ML_for_images.ipynb'\n",
            " Data_Entry_2017.csv\n",
            "'Deeper semi-continous deep_learning_regression.ipynb'\n",
            " deep_learning_regression.ipynb\n",
            "'Even Deeper semi-continous deep_learning_regression.ipynb'\n",
            " Q3Q4\n",
            " Q3Q4.zip\n",
            " scope.ipynb\n",
            "'Semicontious deep_learning_regression.ipynb'\n",
            " tree_based_regression.ipynb\n",
            " Untitled\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "JWyCGhX0dQx3",
        "colab_type": "code",
        "outputId": "f84b6241-a12a-48ed-e63a-6d16e83ec0f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "metadata = pd.read_csv('gdrive/My Drive/Colab Notebooks/Data_Entry_2017.csv')\n",
        "metadata['Finding Labels'].str.contains('\\|').sum()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20796"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "uiY5V4OFhDXa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "20796 images contain multiple labels"
      ]
    },
    {
      "metadata": {
        "id": "q0HLsn3uVOR1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Question 1.2 (5 points)\n",
        "How many patients are there in total? How many patients contributed more than one image? Which patient ID contributed the most images and how many did he or she contribute?"
      ]
    },
    {
      "metadata": {
        "id": "19ppgFt0VRUY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "__Your Answer:__\n",
        "The dataset contains 30,805 unique patients"
      ]
    },
    {
      "metadata": {
        "id": "G9whF_-slD77",
        "colab_type": "code",
        "outputId": "85c8c758-6e29-4501-ce3f-10d035a18785",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        }
      },
      "cell_type": "code",
      "source": [
        "metadata.head(10)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Image Index</th>\n",
              "      <th>Finding Labels</th>\n",
              "      <th>Follow-up #</th>\n",
              "      <th>Patient ID</th>\n",
              "      <th>Patient Age</th>\n",
              "      <th>Patient Gender</th>\n",
              "      <th>View Position</th>\n",
              "      <th>OriginalImage[Width</th>\n",
              "      <th>Height]</th>\n",
              "      <th>OriginalImagePixelSpacing[x</th>\n",
              "      <th>y]</th>\n",
              "      <th>Unnamed: 11</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>00000001_000.png</td>\n",
              "      <td>Cardiomegaly</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>58</td>\n",
              "      <td>M</td>\n",
              "      <td>PA</td>\n",
              "      <td>2682</td>\n",
              "      <td>2749</td>\n",
              "      <td>0.143</td>\n",
              "      <td>0.143</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>00000001_001.png</td>\n",
              "      <td>Cardiomegaly|Emphysema</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>58</td>\n",
              "      <td>M</td>\n",
              "      <td>PA</td>\n",
              "      <td>2894</td>\n",
              "      <td>2729</td>\n",
              "      <td>0.143</td>\n",
              "      <td>0.143</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>00000001_002.png</td>\n",
              "      <td>Cardiomegaly|Effusion</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>58</td>\n",
              "      <td>M</td>\n",
              "      <td>PA</td>\n",
              "      <td>2500</td>\n",
              "      <td>2048</td>\n",
              "      <td>0.168</td>\n",
              "      <td>0.168</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>00000002_000.png</td>\n",
              "      <td>No Finding</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>81</td>\n",
              "      <td>M</td>\n",
              "      <td>PA</td>\n",
              "      <td>2500</td>\n",
              "      <td>2048</td>\n",
              "      <td>0.171</td>\n",
              "      <td>0.171</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>00000003_000.png</td>\n",
              "      <td>Hernia</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>81</td>\n",
              "      <td>F</td>\n",
              "      <td>PA</td>\n",
              "      <td>2582</td>\n",
              "      <td>2991</td>\n",
              "      <td>0.143</td>\n",
              "      <td>0.143</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>00000003_001.png</td>\n",
              "      <td>Hernia</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>74</td>\n",
              "      <td>F</td>\n",
              "      <td>PA</td>\n",
              "      <td>2500</td>\n",
              "      <td>2048</td>\n",
              "      <td>0.168</td>\n",
              "      <td>0.168</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>00000003_002.png</td>\n",
              "      <td>Hernia</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>75</td>\n",
              "      <td>F</td>\n",
              "      <td>PA</td>\n",
              "      <td>2048</td>\n",
              "      <td>2500</td>\n",
              "      <td>0.168</td>\n",
              "      <td>0.168</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>00000003_003.png</td>\n",
              "      <td>Hernia|Infiltration</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>76</td>\n",
              "      <td>F</td>\n",
              "      <td>PA</td>\n",
              "      <td>2698</td>\n",
              "      <td>2991</td>\n",
              "      <td>0.143</td>\n",
              "      <td>0.143</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>00000003_004.png</td>\n",
              "      <td>Hernia</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>77</td>\n",
              "      <td>F</td>\n",
              "      <td>PA</td>\n",
              "      <td>2500</td>\n",
              "      <td>2048</td>\n",
              "      <td>0.168</td>\n",
              "      <td>0.168</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>00000003_005.png</td>\n",
              "      <td>Hernia</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>78</td>\n",
              "      <td>F</td>\n",
              "      <td>PA</td>\n",
              "      <td>2686</td>\n",
              "      <td>2991</td>\n",
              "      <td>0.143</td>\n",
              "      <td>0.143</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        Image Index          Finding Labels  Follow-up #  Patient ID  \\\n",
              "0  00000001_000.png            Cardiomegaly            0           1   \n",
              "1  00000001_001.png  Cardiomegaly|Emphysema            1           1   \n",
              "2  00000001_002.png   Cardiomegaly|Effusion            2           1   \n",
              "3  00000002_000.png              No Finding            0           2   \n",
              "4  00000003_000.png                  Hernia            0           3   \n",
              "5  00000003_001.png                  Hernia            1           3   \n",
              "6  00000003_002.png                  Hernia            2           3   \n",
              "7  00000003_003.png     Hernia|Infiltration            3           3   \n",
              "8  00000003_004.png                  Hernia            4           3   \n",
              "9  00000003_005.png                  Hernia            5           3   \n",
              "\n",
              "   Patient Age Patient Gender View Position  OriginalImage[Width  Height]  \\\n",
              "0           58              M            PA                 2682     2749   \n",
              "1           58              M            PA                 2894     2729   \n",
              "2           58              M            PA                 2500     2048   \n",
              "3           81              M            PA                 2500     2048   \n",
              "4           81              F            PA                 2582     2991   \n",
              "5           74              F            PA                 2500     2048   \n",
              "6           75              F            PA                 2048     2500   \n",
              "7           76              F            PA                 2698     2991   \n",
              "8           77              F            PA                 2500     2048   \n",
              "9           78              F            PA                 2686     2991   \n",
              "\n",
              "   OriginalImagePixelSpacing[x     y]  Unnamed: 11  \n",
              "0                        0.143  0.143          NaN  \n",
              "1                        0.143  0.143          NaN  \n",
              "2                        0.168  0.168          NaN  \n",
              "3                        0.171  0.171          NaN  \n",
              "4                        0.143  0.143          NaN  \n",
              "5                        0.168  0.168          NaN  \n",
              "6                        0.168  0.168          NaN  \n",
              "7                        0.143  0.143          NaN  \n",
              "8                        0.168  0.168          NaN  \n",
              "9                        0.143  0.143          NaN  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "2FUa0y5Rk63b",
        "colab_type": "code",
        "outputId": "af461d3a-dba5-4411-d6c4-9dffc015fe5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "df_count = (metadata.groupby('Patient ID')\n",
        "         .agg({'Patient ID' : 'count'})\n",
        "         .rename(columns={'Patient ID':'count'}))\n",
        "\n",
        "df_count.query('count > 1').shape"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(13302, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "zoVIesdIofpu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "13302 patients contributed more than one image. We can see the patient with the most images below:"
      ]
    },
    {
      "metadata": {
        "id": "EsuGzKK1oizx",
        "colab_type": "code",
        "outputId": "8eab73f0-514b-4551-f446-7c71cc8e2aef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        }
      },
      "cell_type": "code",
      "source": [
        "df_count.sort_values('count', ascending = False).head(1)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Patient ID</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>10007</th>\n",
              "      <td>184</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            count\n",
              "Patient ID       \n",
              "10007         184"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "JyVq0OzcVTfH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Question 2: Check the images (10 points)\n",
        "In the following questions, you will be asked to examine the images in the NIH CXR8 dataset. The images for Question 2.1 and 2.2 could be found at https://www.dropbox.com/sh/2h068ge9xv1g27u/AAAXVq8VYXF6HRlHvzvjy-e6a?dl=0. \n",
        "\n",
        "Feel free to collaborate with other students or consult any references. For example, this (https://lukeoakdenrayner.wordpress.com/2018/01/24/chexnet-an-in-depth-review/) and many other blog posts provide some contexts."
      ]
    },
    {
      "metadata": {
        "id": "AZgIj69yVVhC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Question 2.1 (2 points)\n",
        "What is the NIH-labeled diagnosis of image `00001583_014.png`? What did you see in this image? Word limit: 100 words."
      ]
    },
    {
      "metadata": {
        "id": "nz3w83OSkZDi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "__Your Answer:__\n",
        "The patient has a Pneumothorax in the left lung. We can see a collapsed lung, caused by either a puncture of the thorax, or a spontaneous perforation of a lung bulla. "
      ]
    },
    {
      "metadata": {
        "id": "4A24cNboVXDC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Question 2.2 (3 points)\n",
        "What is the NIH-labeled diagnosis of image `00000019_000.png`? What did you see in this image? Word limit: 100 words."
      ]
    },
    {
      "metadata": {
        "id": "mOC4OgN4kbXL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "__Your Answer:__\n",
        "The image has 3 diagnoses: Atelectasis, Effusion and Pleural_Thickening. \n",
        "\n",
        "We can see signs of Atelectasis ands effusion in the left lung. A good place to see the effusion between the 3 lobes of the left lung is the white triangle in the center of the left lung. \n",
        "I can not identify direct signs of pleural thickening, but it is not a suprising co-diagnosis, given the clnical context. The patient has effusions and, given the sternal cerclage, may had open chest surgery. "
      ]
    },
    {
      "metadata": {
        "id": "waQr4FscVZOQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Question 2.3 (5 points)\n",
        "What is \"View Position?\" How does it affect the resulting chest X-ray images visually? Word limit: 100 words.\n"
      ]
    },
    {
      "metadata": {
        "id": "EK3dNZigkcE7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "__Your Answer:__\n",
        "\n",
        "Most images were collected PA, that is posteroanterior. Images that are collected this way are collected with the radiation-source coming from the back of the patient. This way, and because the heart is closer to the front of the chest wall, the heart contour is larger in these images. In contrast, AP images, that are collected from sick patients which can not stand in front of a X-ray detector themselves, show smaller heart contours."
      ]
    },
    {
      "metadata": {
        "id": "pSSXlbA7VbLo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Question 3: Build a custom convolutional neural network (15 points)\n",
        "For this question, we ask you to build a multi-layer convolutional neural network to classify a subset of cardiomegaly images from normal ones. Please download the training set and the validation set here (https://www.dropbox.com/sh/ojiw79q8786ua4x/AAAtaJVKEdv91Zybpi-fAfMsa?dl=0). Please DO NOT use any other image from NIH CXR8 or from other databases for this question. Feel free to use keras or any other high-level deep learning packages to classify the images.\n",
        "\n",
        "Design a convolutional neural network with at least two convolution layers, at least one max-pooling layer, and at least one dropout layer. Although you should explore various combinations of hyperparameters, we will grade this question based on the accuracy of the implementation, not the performance of the network.\n",
        "\n",
        "What is your design? What binary loss/accuracy did you get in the training and validation set? Please include your code in the assignment submission.\n",
        "\n",
        "Your neural network should have an AUC >= 0.55 when evaluated by the validation data set."
      ]
    },
    {
      "metadata": {
        "id": "0AXxlAAQc3S1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Pre-process data"
      ]
    },
    {
      "metadata": {
        "id": "mueSBuaJvQZ5",
        "colab_type": "code",
        "outputId": "6ad6c408-5afe-47e8-90d8-c5f815c97de1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# I list my image data\n",
        "! ls gdrive/'My Drive'/'Colab Notebooks'/Q3Q4/converted_to_RGB"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train  train.tar  val  val.tar\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "aPgMXhHPhZS1",
        "colab_type": "code",
        "outputId": "5eb8ee04-6ec2-46f0-dc40-3f3bb67b3060",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# and I list my label information\n",
        "! ls gdrive/'My Drive'/'Colab Notebooks'/Q3Q4\n",
        "y_train = pd.read_csv('gdrive/My Drive/Colab Notebooks/Q3Q4/train.csv', header=None, names = ['image', 'label'])\n",
        "y_test = pd.read_csv('gdrive/My Drive/Colab Notebooks/Q3Q4/val.csv', header=None, names = ['image', 'label'])\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "converted_to_RGB  train.csv  val.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TDpMIGAWulsN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#! mkdir gdrive/My\\ Drive/Colab\\ Notebooks/Q3Q4/converted_to_RGB/train/Cardiomegaly\n",
        "#! mkdir gdrive/My\\ Drive/Colab\\ Notebooks/Q3Q4/converted_to_RGB/train/No\\ Finding\n",
        "#! mkdir gdrive/My\\ Drive/Colab\\ Notebooks/Q3Q4/converted_to_RGB/val/Cardiomegaly\n",
        "#! mkdir gdrive/My\\ Drive/Colab\\ Notebooks/Q3Q4/converted_to_RGB/val/No\\ Finding\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YnbFNBW-im5p",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "\n",
        "y_train['full_path'] = 'gdrive/My Drive/Colab Notebooks/Q3Q4/converted_to_RGB/train/' + y_train['image']\n",
        "y_test['full_path'] = 'gdrive/My Drive/Colab Notebooks/Q3Q4/converted_to_RGB/val/' + y_test['image']\n",
        "\n",
        "y_train['destination_path'] = 'gdrive/My Drive/Colab Notebooks/Q3Q4/converted_to_RGB/train/' + y_train['label'] + '/' + y_train['image']\n",
        "y_test['destination_path'] = 'gdrive/My Drive/Colab Notebooks/Q3Q4/converted_to_RGB/val/' + y_test['label'] + '/' + y_test['image']\n",
        "\n",
        "# only run once\n",
        "#y_train.apply(lambda row: shutil.move(row['full_path'], row['destination_path']), axis=1)\n",
        "#y_test.apply(lambda row: shutil.move(row['full_path'], row['destination_path']), axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rh3YKsoZvuMp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 952
        },
        "outputId": "5176f4ef-8b35-4dc6-8e29-ae2527da7b8e"
      },
      "cell_type": "code",
      "source": [
        "! ls gdrive/My\\ Drive/Colab\\ Notebooks/Q3Q4/converted_to_RGB/val/Cardiomegaly"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "00000001_000.png  00005035_004.png  00012463_000.png  00020754_000.png\n",
            "00000013_045.png  00005064_013.png  00012523_000.png  00020839_002.png\n",
            "00000032_000.png  00005066_065.png  00012707_000.png  00020979_001.png\n",
            "00000032_053.png  00005270_007.png  00012798_000.png  00021032_000.png\n",
            "00000045_000.png  00005425_000.png  00012845_001.png  00021058_000.png\n",
            "00000155_000.png  00005532_007.png  00012987_002.png  00021186_001.png\n",
            "00000176_001.png  00005641_001.png  00012987_012.png  00021326_000.png\n",
            "00000211_000.png  00005906_000.png  00013409_001.png  00021373_010.png\n",
            "00000271_004.png  00006068_002.png  00013522_000.png  00021790_000.png\n",
            "00000287_000.png  00006129_000.png  00013876_001.png  00021818_023.png\n",
            "00000330_000.png  00006341_000.png  00013876_006.png  00021895_007.png\n",
            "00000338_000.png  00006411_017.png  00013876_007.png  00021958_000.png\n",
            "00000374_000.png  00006481_015.png  00013992_017.png  00022147_005.png\n",
            "00000547_005.png  00006537_000.png  00014022_084.png  00022945_000.png\n",
            "00000825_003.png  00006875_003.png  00014273_000.png  00023271_010.png\n",
            "00001373_009.png  00006875_004.png  00014287_000.png  00023325_000.png\n",
            "00001373_012.png  00006894_000.png  00014449_001.png  00023325_026.png\n",
            "00001373_013.png  00007018_014.png  00014477_000.png  00023325_031.png\n",
            "00001582_012.png  00007326_000.png  00014637_010.png  00023452_000.png\n",
            "00001959_000.png  00007393_000.png  00014692_000.png  00023768_000.png\n",
            "00002103_001.png  00007428_004.png  00014706_029.png  00024460_000.png\n",
            "00002256_012.png  00007735_031.png  00014861_000.png  00025126_000.png\n",
            "00002743_006.png  00007735_038.png  00014962_001.png  00025382_000.png\n",
            "00002798_003.png  00007735_040.png  00015166_000.png  00025416_001.png\n",
            "00002889_000.png  00007921_000.png  00015286_001.png  00025684_000.png\n",
            "00003015_000.png  00008226_004.png  00015410_000.png  00025732_004.png\n",
            "00003088_000.png  00008339_000.png  00016034_002.png  00026256_000.png\n",
            "00003091_000.png  00008649_000.png  00016202_000.png  00026357_000.png\n",
            "00003283_000.png  00008871_001.png  00016282_000.png  00026522_000.png\n",
            "00003361_006.png  00009050_000.png  00016322_000.png  00027292_000.png\n",
            "00003410_000.png  00009196_001.png  00016361_000.png  00027779_000.png\n",
            "00003414_000.png  00009257_000.png  00016589_000.png  00027797_000.png\n",
            "00003483_000.png  00009608_024.png  00016606_000.png  00027953_000.png\n",
            "00003700_001.png  00009734_000.png  00016802_000.png  00028005_000.png\n",
            "00003989_003.png  00009745_000.png  00017417_000.png  00028018_000.png\n",
            "00003990_000.png  00009932_004.png  00017448_001.png  00028216_000.png\n",
            "00004007_002.png  00010007_066.png  00017511_011.png  00028400_001.png\n",
            "00004251_000.png  00010437_002.png  00017627_000.png  00028412_000.png\n",
            "00004342_020.png  00010437_003.png  00017688_000.png  00028607_000.png\n",
            "00004344_007.png  00010524_000.png  00017705_004.png  00028646_000.png\n",
            "00004381_047.png  00010638_001.png  00017953_000.png  00028871_001.png\n",
            "00004452_000.png  00010968_000.png  00017980_000.png  00029108_000.png\n",
            "00004533_018.png  00010994_005.png  00017980_001.png  00029246_000.png\n",
            "00004534_000.png  00011010_011.png  00018181_001.png  00029472_000.png\n",
            "00004725_000.png  00011018_001.png  00018233_004.png  00029766_000.png\n",
            "00004728_001.png  00011023_003.png  00018614_004.png  00029794_000.png\n",
            "00004775_000.png  00011139_000.png  00018614_009.png  00029805_003.png\n",
            "00004822_007.png  00011322_002.png  00019486_000.png  00029876_002.png\n",
            "00004822_051.png  00011463_000.png  00019636_000.png  00029906_000.png\n",
            "00004843_000.png  00011463_001.png  00019702_001.png  00030276_000.png\n",
            "00004847_000.png  00011507_000.png  00019801_000.png  00030490_000.png\n",
            "00004857_011.png  00012006_000.png  00020042_006.png  00030544_000.png\n",
            "00004893_063.png  00012150_000.png  00020107_000.png  00030608_000.png\n",
            "00004893_084.png  00012250_001.png  00020564_001.png  00030764_000.png\n",
            "00005035_003.png  00012275_000.png  00020588_000.png\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8bZetloTpyvb",
        "colab_type": "code",
        "outputId": "7469e22c-dda5-4c03-a14a-99de3d71cc30",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "# I create a data-generator for my custom network\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "        rescale=1./255,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        #samplewise_center=True, \n",
        "        #samplewise_std_normalization=True,\n",
        "        horizontal_flip=False) # The heart is always on the left side\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)#,\n",
        "                                  #samplewise_center=True, \n",
        "                                  #samplewise_std_normalization=True)\n",
        "\n",
        "# define the batch size, if there is sufficient GPU memory, you can increase the batch size\n",
        "batch_size = 16\n",
        "\n",
        "# I define the training data generator\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        'gdrive/My Drive/Colab Notebooks/Q3Q4/converted_to_RGB/train',  # the directory for the training data\n",
        "        target_size=(150, 150),  # resize the input images to accommodate the model\n",
        "        batch_size=batch_size,\n",
        "        class_mode='binary')\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "        'gdrive/My Drive/Colab Notebooks/Q3Q4/converted_to_RGB/val',  # the directory for the training data\n",
        "        target_size=(150, 150),  # resize the input images to accommodate the model\n",
        "        batch_size=batch_size,\n",
        "        class_mode='binary')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 1750 images belonging to 2 classes.\n",
            "Found 437 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UpOYsXisdI8j",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Train custom network"
      ]
    },
    {
      "metadata": {
        "id": "ZPCOhKtddFSu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# loading packages\n",
        "################################################################################\n",
        "from keras.preprocessing import image\n",
        "from keras.models import Model, Sequential\n",
        "from keras.layers import Dense, GlobalAveragePooling2D, Conv2D, Flatten, Dropout, MaxPooling2D\n",
        "\n",
        "# setting variables\n",
        "batch_size = 16\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iHob5vsowzl-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Design a convolutional neural network with at least two convolution layers, at least one max-pooling layer, and at least one dropout layer\n",
        "model = Sequential()\n",
        "\n",
        "# adding layers \n",
        "model.add(Conv2D(64, kernel_size=3, activation='relu', input_shape=(150, 150, 3)))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Conv2D(64, kernel_size=3, activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Conv2D(32, kernel_size=3, activation='relu'))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "#model.add(Dense(16, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "# compiling the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# train the model\n",
        "model.fit_generator(train_generator,\n",
        "                    epochs=10, \n",
        "                    steps_per_epoch=1750 // batch_size,\n",
        "                    validation_data=test_generator,\n",
        "                    validation_steps=437 // batch_size)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hLCE3fr9JDTn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bc906ef9-7984-4fa0-ad4d-08e59a26e560"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# I measure the networks AUC on the validation dataset. I called this dataset test dataset, which it isn't. \n",
        "# I create a dedicated generator without randomization and a batchsize of 1\n",
        "generator = test_datagen.flow_from_directory(\n",
        "        'gdrive/My Drive/Colab Notebooks/Q3Q4/converted_to_RGB/val',\n",
        "        target_size=(150, 150),\n",
        "        batch_size=1,\n",
        "        class_mode=None,\n",
        "        shuffle=False)\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 437 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4y7c8vdrge7F",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# I calculate the class probabilities\n",
        "probabilities = model.predict_generator(generator, steps = 437)\n",
        "\n",
        "\n",
        "# Suprisingly, I do not need to binarize the class labels first\n",
        "roc_auc_score(y_test['label'], probabilities)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xkXSpQihVjUB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "__Your Answer:__\n",
        "\n",
        "My final network consists of 3 convolutional layers that are interspersed with Max-pooling layers. The data is flattened and then processed in two fully connected layers. One of the fully connected layers is dropout regularized. \n",
        "\n",
        "The accuracy and loss during training reached up to 0.59 and 0.69, repsectively. The metrics on the validation data reached up to 0.65 and 0.69, respectively. \n",
        "\n",
        "The AUROC on the validation data is larger than 0.55."
      ]
    },
    {
      "metadata": {
        "id": "0wsYHswwVlYn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Question 4: Transfer learning: Using the VGGNet (16 layers) architecture (20 points)\n",
        "For this question, we ask you to employ VGGNet, a convolutional neural network built for ImageNet, to classify the same subset of cardiomegaly images from normal ones (https://www.dropbox.com/sh/ojiw79q8786ua4x/AAAtaJVKEdv91Zybpi-fAfMsa?dl=0). We encourage you to take a look at the documentation for keras.applications (https://keras.io/applications/) and reuse their modules. Please DO NOT use any other images from NIH CXR8 or from other databases for this question. Although you should explore various combinations of hyperparameters, we will grade this question based on the accuracy of the implementation, not the performance of the network.\n",
        "\n",
        "Your neural network should have an AUC >= 0.7 when evaluated by the validation data set."
      ]
    },
    {
      "metadata": {
        "id": "TT_rg5qxpcBh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "89142d79-e63c-4a0f-a1c5-009da9c7861c"
      },
      "cell_type": "code",
      "source": [
        "# First, I generate my training data in a new way, that is independent of the image generator. I struggled with fine-tuning the network using generators, and resorted to manually importing the information\n",
        "!tar -xf gdrive/My\\ Drive/Colab\\ Notebooks/Q3Q4/converted_to_RGB/train.tar\n",
        "!tar -xf gdrive/My\\ Drive/Colab\\ Notebooks/Q3Q4/converted_to_RGB/val.tar\n",
        "\n",
        "!ls gdrive/My\\ Drive/Colab\\ Notebooks/Q3Q4/converted_to_RGB/"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train_cat  train.tar  val_cat  val.tar\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Q5_caaWxqIKe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# I import packages\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from PIL import Image\n",
        "from skimage import color\n",
        "from skimage import io\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import resample\n",
        "from sklearn.preprocessing import scale\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "\n",
        "##################\n",
        "# TRAIN data\n",
        "# read in the labels\n",
        "train_labels = pd.read_csv('gdrive/My Drive/Colab Notebooks/Q3Q4/train.csv',  header=None, index_col=0)\n",
        "# Any results you write to the current directory are saved as output.\n",
        "# I resize the images to make them smaller\n",
        "image_size = (224,224)\n",
        "\n",
        "# read in the training images\n",
        "train_images = []\n",
        "train_dir = 'gdrive/My Drive/Colab Notebooks/Q3Q4/converted_to_RGB/train/'\n",
        "\n",
        "train_files = train_labels.index.values.tolist()\n",
        "\n",
        "for f in train_files:\n",
        "  img = Image.open(train_dir + f)\n",
        "  img = img.resize(image_size)\n",
        "  img_arr = np.array(img)\n",
        "  train_images.append(img_arr)\n",
        "\n",
        "train_X = np.array(train_images)\n",
        "# reorder the labels so that they line up with the order of the image files\n",
        "train_labels = train_labels.reindex(train_files)\n",
        "print(train_X.shape)\n",
        "\n",
        "\n",
        "# I scale the images across the full dataset\n",
        "train_X = train_X/255\n",
        "train_X = (train_X - train_X.mean(axis=0))/train_X.std(axis=0)\n",
        "\n",
        "train_X = color.gray2rgb(train_X)\n",
        "train_X.shape\n",
        "\n",
        "# I create binary labels for my y variable\n",
        "label_transformer = LabelBinarizer()\n",
        "train_y = label_transformer.fit_transform(train_labels)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wvFtijdX8gBs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "20ebf1e8-1880-4861-ddbf-017dd53d40f9"
      },
      "cell_type": "code",
      "source": [
        "#####################\n",
        "# VALIDATION data\n",
        "# read in the labels\n",
        "val_labels = pd.read_csv('gdrive/My Drive/Colab Notebooks/Q3Q4/val.csv',  header=None, index_col=0)\n",
        "\n",
        "# read in the training images\n",
        "val_images = []\n",
        "val_dir = 'gdrive/My Drive/Colab Notebooks/Q3Q4/converted_to_RGB/val/'\n",
        "\n",
        "val_files = val_labels.index.values.tolist()\n",
        "\n",
        "for f in val_files:\n",
        "  img = Image.open(val_dir + f)\n",
        "  img = img.resize(image_size)\n",
        "  img_arr = np.array(img)\n",
        "  val_images.append(img_arr)\n",
        "\n",
        "val_X = np.array(val_images)\n",
        "# reorder the labels so that they line up with the order of the image files\n",
        "val_labels = val_labels.reindex(val_files)\n",
        "print(val_X.shape)\n",
        "\n",
        "\n",
        "# I scale the images across the full dataset\n",
        "val_X = val_X/255\n",
        "val_X = (val_X - val_X.mean(axis=0))/val_X.std(axis=0)\n",
        "\n",
        "val_X = color.gray2rgb(val_X)\n",
        "val_X.shape\n",
        "\n",
        "# I create binary labels for my y variable\n",
        "label_transformer = LabelBinarizer()\n",
        "val_y = label_transformer.fit_transform(val_labels)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(437, 224, 224)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "kICXlS7tVnLJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Question 4.1 (10 points)\n",
        "What is your best validation accuracy of fine-tuning a 16-layer VGGNet WITHOUT ImageNet weights? In your model with the lowest validation loss, what are the hyperparameters? What is the validation loss? What is the training loss/accuracy? Please include your code in the assignment submission."
      ]
    },
    {
      "metadata": {
        "id": "SfxJ3JFEkiv4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1241
        },
        "outputId": "a6e74cd5-759b-4d00-8916-4a2cffc1aee8"
      },
      "cell_type": "code",
      "source": [
        "## I take inspiration from the in-class assignment for this code\n",
        "################################################################################\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.applications.vgg16 import preprocess_input\n",
        "from keras.optimizers import Adam,SGD,RMSprop\n",
        "from keras.preprocessing import image\n",
        "from keras.models import Model, Sequential\n",
        "from keras.layers import Dense, GlobalAveragePooling2D, Conv2D, Flatten, Dropout, MaxPooling2D\n",
        "\n",
        "\n",
        "base_model = VGG16(weights=None, include_top=True) #different from the in-class excercise, I keep the classification layer and just add another set of layers on top.\n",
        "# get the output of the base model\n",
        "x = base_model.output\n",
        "# add a 2D global average pooling layer\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "# I add a regularizing layer\n",
        "x = Dropout(0.5)(x)\n",
        "# add a layer for binary classification. As above, I use a sigmoid activation function\n",
        "predictions = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "model_vvg_random = Model(inputs=base_model.input, outputs=predictions)\n",
        "    \n",
        "# I compile the model and start training using the previously defined generators\n",
        "# I keep the optimizer and loss function constant for consistency\n",
        "model_vvg_random.compile(optimizer=RMSprop(lr=1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train model with same schedule, I did not freeze any layers, as the weights are all random\n",
        "model_vvg_random.fit(train_X,train_y, validation_data=[val_X,val_y], epochs=35,batch_size=32)\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 1750 samples, validate on 437 samples\n",
            "Epoch 1/35\n",
            "1750/1750 [==============================] - 35s 20ms/step - loss: 0.6932 - acc: 0.4960 - val_loss: 0.6931 - val_acc: 0.4989\n",
            "Epoch 2/35\n",
            "1750/1750 [==============================] - 32s 18ms/step - loss: 0.6932 - acc: 0.4886 - val_loss: 0.6931 - val_acc: 0.4989\n",
            "Epoch 3/35\n",
            "1750/1750 [==============================] - 31s 18ms/step - loss: 0.6906 - acc: 0.5680 - val_loss: 0.6890 - val_acc: 0.6110\n",
            "Epoch 4/35\n",
            "1750/1750 [==============================] - 32s 18ms/step - loss: 0.6862 - acc: 0.6046 - val_loss: 0.6888 - val_acc: 0.5789\n",
            "Epoch 5/35\n",
            "1750/1750 [==============================] - 32s 18ms/step - loss: 0.6833 - acc: 0.6246 - val_loss: 0.6852 - val_acc: 0.6133\n",
            "Epoch 6/35\n",
            "1750/1750 [==============================] - 32s 18ms/step - loss: 0.6828 - acc: 0.6097 - val_loss: 0.6832 - val_acc: 0.6201\n",
            "Epoch 7/35\n",
            "1750/1750 [==============================] - 32s 18ms/step - loss: 0.6807 - acc: 0.6320 - val_loss: 0.6846 - val_acc: 0.6064\n",
            "Epoch 8/35\n",
            "1750/1750 [==============================] - 32s 18ms/step - loss: 0.6774 - acc: 0.6509 - val_loss: 0.6835 - val_acc: 0.6018\n",
            "Epoch 9/35\n",
            "1750/1750 [==============================] - 32s 18ms/step - loss: 0.6771 - acc: 0.6394 - val_loss: 0.6830 - val_acc: 0.5881\n",
            "Epoch 10/35\n",
            "1750/1750 [==============================] - 32s 18ms/step - loss: 0.6754 - acc: 0.6383 - val_loss: 0.6800 - val_acc: 0.6110\n",
            "Epoch 11/35\n",
            "1750/1750 [==============================] - 32s 18ms/step - loss: 0.6745 - acc: 0.6497 - val_loss: 0.6771 - val_acc: 0.6224\n",
            "Epoch 12/35\n",
            "1750/1750 [==============================] - 32s 18ms/step - loss: 0.6707 - acc: 0.6634 - val_loss: 0.6803 - val_acc: 0.5973\n",
            "Epoch 13/35\n",
            "1750/1750 [==============================] - 32s 18ms/step - loss: 0.6700 - acc: 0.6526 - val_loss: 0.6762 - val_acc: 0.6133\n",
            "Epoch 14/35\n",
            "1750/1750 [==============================] - 32s 18ms/step - loss: 0.6661 - acc: 0.6686 - val_loss: 0.6742 - val_acc: 0.6178\n",
            "Epoch 15/35\n",
            "1750/1750 [==============================] - 31s 18ms/step - loss: 0.6675 - acc: 0.6514 - val_loss: 0.6763 - val_acc: 0.6133\n",
            "Epoch 16/35\n",
            "1750/1750 [==============================] - 31s 18ms/step - loss: 0.6688 - acc: 0.6486 - val_loss: 0.6797 - val_acc: 0.5995\n",
            "Epoch 17/35\n",
            "1750/1750 [==============================] - 31s 18ms/step - loss: 0.6653 - acc: 0.6571 - val_loss: 0.6858 - val_acc: 0.5652\n",
            "Epoch 18/35\n",
            "1750/1750 [==============================] - 31s 18ms/step - loss: 0.6630 - acc: 0.6663 - val_loss: 0.6807 - val_acc: 0.5950\n",
            "Epoch 19/35\n",
            "1750/1750 [==============================] - 31s 18ms/step - loss: 0.6693 - acc: 0.6331 - val_loss: 0.6785 - val_acc: 0.5904\n",
            "Epoch 20/35\n",
            "1750/1750 [==============================] - 31s 18ms/step - loss: 0.6599 - acc: 0.6623 - val_loss: 0.6710 - val_acc: 0.6156\n",
            "Epoch 21/35\n",
            "1750/1750 [==============================] - 31s 18ms/step - loss: 0.6562 - acc: 0.6726 - val_loss: 0.6711 - val_acc: 0.6178\n",
            "Epoch 22/35\n",
            "1750/1750 [==============================] - 31s 18ms/step - loss: 0.6475 - acc: 0.7023 - val_loss: 0.6691 - val_acc: 0.6178\n",
            "Epoch 23/35\n",
            "1750/1750 [==============================] - 31s 18ms/step - loss: 0.6479 - acc: 0.6971 - val_loss: 0.6703 - val_acc: 0.6087\n",
            "Epoch 24/35\n",
            "1750/1750 [==============================] - 31s 18ms/step - loss: 0.6514 - acc: 0.6794 - val_loss: 0.6641 - val_acc: 0.6316\n",
            "Epoch 25/35\n",
            "1750/1750 [==============================] - 31s 18ms/step - loss: 0.6465 - acc: 0.6931 - val_loss: 0.6739 - val_acc: 0.6041\n",
            "Epoch 26/35\n",
            "1750/1750 [==============================] - 31s 18ms/step - loss: 0.6517 - acc: 0.6674 - val_loss: 0.6723 - val_acc: 0.5950\n",
            "Epoch 27/35\n",
            "1750/1750 [==============================] - 31s 18ms/step - loss: 0.6484 - acc: 0.6766 - val_loss: 0.6600 - val_acc: 0.6339\n",
            "Epoch 28/35\n",
            "1750/1750 [==============================] - 31s 18ms/step - loss: 0.6409 - acc: 0.6949 - val_loss: 0.6810 - val_acc: 0.5835\n",
            "Epoch 29/35\n",
            "1750/1750 [==============================] - 31s 18ms/step - loss: 0.6427 - acc: 0.6903 - val_loss: 0.6808 - val_acc: 0.5789\n",
            "Epoch 30/35\n",
            "1750/1750 [==============================] - 31s 18ms/step - loss: 0.6456 - acc: 0.6783 - val_loss: 0.6713 - val_acc: 0.6041\n",
            "Epoch 31/35\n",
            "1750/1750 [==============================] - 31s 18ms/step - loss: 0.6417 - acc: 0.6897 - val_loss: 0.6760 - val_acc: 0.5858\n",
            "Epoch 32/35\n",
            "1750/1750 [==============================] - 31s 18ms/step - loss: 0.6383 - acc: 0.6891 - val_loss: 0.6696 - val_acc: 0.5995\n",
            "Epoch 33/35\n",
            "1750/1750 [==============================] - 31s 18ms/step - loss: 0.6398 - acc: 0.6829 - val_loss: 0.6711 - val_acc: 0.6064\n",
            "Epoch 34/35\n",
            "1750/1750 [==============================] - 31s 18ms/step - loss: 0.6367 - acc: 0.6926 - val_loss: 0.6685 - val_acc: 0.6087\n",
            "Epoch 35/35\n",
            "1750/1750 [==============================] - 31s 18ms/step - loss: 0.6376 - acc: 0.6851 - val_loss: 0.6655 - val_acc: 0.6201\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f8040393470>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "metadata": {
        "id": "3rZ10MbGS9q5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "faff383b-942c-43c1-8318-1e41b89badd6"
      },
      "cell_type": "code",
      "source": [
        "pred_y = model_vvg_random.predict(val_X)\n",
        "print(\"The AUROC on the validation data is \", roc_auc_score(val_y, pred_y))\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The AUROC on the validation data is  0.6324724561183025\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "N_Yh0KVjVqrS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "__Your Answer:__\n",
        "\n",
        "For the VGGNet architecture with random initializations, the network does learn to differentiate between the two classes when trained with a different set of hyperparameters. However, the model performance after 35 epochs is still below AURC = 0.7. A possible reason for this could be that the VGGNet achitecture has too many parameters, that can not converge to a robust configuration with the limited available training data and training iterations."
      ]
    },
    {
      "metadata": {
        "id": "nsnPBMwIVsho",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Question 4.2 (10 points)\n",
        "What is your best validation accuracy of fine-tuning a 16-layer VGGNet WITH ImageNet weights? In your model with the lowest validation loss, what are the hyperparameters? What is the validation loss? What is the training loss/accuracy? Please include your code in the assignment submission."
      ]
    },
    {
      "metadata": {
        "id": "wKNqvD_9Vt8e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1241
        },
        "outputId": "b8ca641a-7867-4ddf-f6d2-d4fb6b28c9d7"
      },
      "cell_type": "code",
      "source": [
        "## I keep everyhting the same as above, but load the imagenet features\n",
        "################################################################################\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.applications.vgg16 import preprocess_input\n",
        "\n",
        "model = VGG16(weights='imagenet', include_top=True)\n",
        "# get the output of the base model\n",
        "x = base_model.output\n",
        "# get the output of the base model\n",
        "x = base_model.output\n",
        "# add a 2D global average pooling layer\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "# I add a regularizing layer\n",
        "x = Dropout(0.5)(x)\n",
        "# add a layer for binary classification. As above, I use a sigmoid activation function\n",
        "predictions = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "model_vvg_imagenet = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "\n",
        "    \n",
        "# I compile the model and start training using the previously defined generators\n",
        "# I keep the optimizer and loss function constant for consistency\n",
        "model_vvg_imagenet.compile(optimizer=RMSprop(lr=1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train model with same schedule, I did not freeze any layers, as the weights are all random\n",
        "model_vvg_random.fit(train_X,train_y, validation_data=[val_X,val_y], epochs=35,batch_size=32)\n",
        "\n",
        "pred_y_inception = model_vvg_random.predict(val_X)\n",
        "\n",
        "print(\"The AUROC on the validation data is \", roc_auc_score(val_y, pred_y_inception))\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 1750 samples, validate on 437 samples\n",
            "Epoch 1/35\n",
            "1750/1750 [==============================] - 32s 18ms/step - loss: 0.6509 - acc: 0.6811 - val_loss: 0.6603 - val_acc: 0.6476\n",
            "Epoch 2/35\n",
            "1750/1750 [==============================] - 32s 18ms/step - loss: 0.6504 - acc: 0.6766 - val_loss: 0.6628 - val_acc: 0.6339\n",
            "Epoch 3/35\n",
            "1750/1750 [==============================] - 31s 18ms/step - loss: 0.6408 - acc: 0.7057 - val_loss: 0.6581 - val_acc: 0.6476\n",
            "Epoch 4/35\n",
            "1750/1750 [==============================] - 31s 18ms/step - loss: 0.6426 - acc: 0.6931 - val_loss: 0.6613 - val_acc: 0.6339\n",
            "Epoch 5/35\n",
            "1750/1750 [==============================] - 31s 18ms/step - loss: 0.6350 - acc: 0.7120 - val_loss: 0.6598 - val_acc: 0.6362\n",
            "Epoch 6/35\n",
            "1750/1750 [==============================] - 31s 18ms/step - loss: 0.6302 - acc: 0.7206 - val_loss: 0.6568 - val_acc: 0.6430\n",
            "Epoch 7/35\n",
            "1750/1750 [==============================] - 31s 18ms/step - loss: 0.6409 - acc: 0.6840 - val_loss: 0.6728 - val_acc: 0.5995\n",
            "Epoch 8/35\n",
            "1750/1750 [==============================] - 31s 18ms/step - loss: 0.6438 - acc: 0.6783 - val_loss: 0.6610 - val_acc: 0.6293\n",
            "Epoch 9/35\n",
            "1750/1750 [==============================] - 31s 18ms/step - loss: 0.6352 - acc: 0.6983 - val_loss: 0.6540 - val_acc: 0.6430\n",
            "Epoch 10/35\n",
            "1750/1750 [==============================] - 31s 18ms/step - loss: 0.6300 - acc: 0.7063 - val_loss: 0.6563 - val_acc: 0.6384\n",
            "Epoch 11/35\n",
            "1750/1750 [==============================] - 31s 18ms/step - loss: 0.6172 - acc: 0.7320 - val_loss: 0.6569 - val_acc: 0.6339\n",
            "Epoch 12/35\n",
            "1750/1750 [==============================] - 31s 18ms/step - loss: 0.6287 - acc: 0.7074 - val_loss: 0.6687 - val_acc: 0.6110\n",
            "Epoch 13/35\n",
            "1750/1750 [==============================] - 31s 18ms/step - loss: 0.6238 - acc: 0.7131 - val_loss: 0.6601 - val_acc: 0.6247\n",
            "Epoch 14/35\n",
            "1750/1750 [==============================] - 31s 18ms/step - loss: 0.6252 - acc: 0.7086 - val_loss: 0.6603 - val_acc: 0.6270\n",
            "Epoch 15/35\n",
            "1750/1750 [==============================] - 31s 18ms/step - loss: 0.6227 - acc: 0.7126 - val_loss: 0.6508 - val_acc: 0.6453\n",
            "Epoch 16/35\n",
            "1750/1750 [==============================] - 32s 18ms/step - loss: 0.6140 - acc: 0.7263 - val_loss: 0.6544 - val_acc: 0.6407\n",
            "Epoch 17/35\n",
            "1750/1750 [==============================] - 31s 18ms/step - loss: 0.6179 - acc: 0.7131 - val_loss: 0.6578 - val_acc: 0.6316\n",
            "Epoch 18/35\n",
            "1750/1750 [==============================] - 31s 18ms/step - loss: 0.6099 - acc: 0.7269 - val_loss: 0.6452 - val_acc: 0.6568\n",
            "Epoch 19/35\n",
            "1750/1750 [==============================] - 31s 18ms/step - loss: 0.6089 - acc: 0.7291 - val_loss: 0.6608 - val_acc: 0.6247\n",
            "Epoch 20/35\n",
            "1750/1750 [==============================] - 31s 18ms/step - loss: 0.6087 - acc: 0.7280 - val_loss: 0.6458 - val_acc: 0.6545\n",
            "Epoch 21/35\n",
            "1750/1750 [==============================] - 32s 18ms/step - loss: 0.6098 - acc: 0.7217 - val_loss: 0.6635 - val_acc: 0.6201\n",
            "Epoch 22/35\n",
            "1750/1750 [==============================] - 31s 18ms/step - loss: 0.5995 - acc: 0.7394 - val_loss: 0.6457 - val_acc: 0.6522\n",
            "Epoch 23/35\n",
            "1750/1750 [==============================] - 32s 18ms/step - loss: 0.6034 - acc: 0.7297 - val_loss: 0.6649 - val_acc: 0.6178\n",
            "Epoch 24/35\n",
            "1750/1750 [==============================] - 32s 18ms/step - loss: 0.6100 - acc: 0.7137 - val_loss: 0.6433 - val_acc: 0.6590\n",
            "Epoch 25/35\n",
            "1750/1750 [==============================] - 32s 18ms/step - loss: 0.5905 - acc: 0.7451 - val_loss: 0.6491 - val_acc: 0.6453\n",
            "Epoch 26/35\n",
            "1750/1750 [==============================] - 32s 18ms/step - loss: 0.5911 - acc: 0.7451 - val_loss: 0.6387 - val_acc: 0.6636\n",
            "Epoch 27/35\n",
            "1750/1750 [==============================] - 32s 18ms/step - loss: 0.6005 - acc: 0.7257 - val_loss: 0.6330 - val_acc: 0.6728\n",
            "Epoch 28/35\n",
            "1750/1750 [==============================] - 32s 18ms/step - loss: 0.5763 - acc: 0.7663 - val_loss: 0.6403 - val_acc: 0.6590\n",
            "Epoch 29/35\n",
            "1750/1750 [==============================] - 32s 18ms/step - loss: 0.5835 - acc: 0.7503 - val_loss: 0.6364 - val_acc: 0.6636\n",
            "Epoch 30/35\n",
            "1750/1750 [==============================] - 32s 18ms/step - loss: 0.5705 - acc: 0.7651 - val_loss: 0.6397 - val_acc: 0.6499\n",
            "Epoch 31/35\n",
            "1750/1750 [==============================] - 32s 18ms/step - loss: 0.5767 - acc: 0.7554 - val_loss: 0.6315 - val_acc: 0.6728\n",
            "Epoch 32/35\n",
            "1750/1750 [==============================] - 32s 18ms/step - loss: 0.5582 - acc: 0.7840 - val_loss: 0.6245 - val_acc: 0.6796\n",
            "Epoch 33/35\n",
            "1750/1750 [==============================] - 32s 18ms/step - loss: 0.5611 - acc: 0.7749 - val_loss: 0.6420 - val_acc: 0.6590\n",
            "Epoch 34/35\n",
            "1750/1750 [==============================] - 32s 18ms/step - loss: 0.5531 - acc: 0.7834 - val_loss: 0.6303 - val_acc: 0.6728\n",
            "Epoch 35/35\n",
            "1750/1750 [==============================] - 32s 18ms/step - loss: 0.5426 - acc: 0.7949 - val_loss: 0.6331 - val_acc: 0.6659\n",
            "The AUROC on the validation data is  0.7190628796447573\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-6mlL3eGklj3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "__Your Answer:__\n",
        "\n",
        "When using the pre-trained parameters from the ImageNet-optimized VGGNet and the same hyperparameter configuration we can observe an AUROC of > 0.7. With the limited number of training data and training epochs, the model was still able to reach a higher performance, because the model parameters were already pre-optimized. "
      ]
    },
    {
      "metadata": {
        "id": "USEauOuqVwMa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Question 5: Multi-class classification and the BMI707 Kaggle contest (20 points)\n",
        "In this question, we will build multi-class classifiers to distinguish different types of lung diseases using the NIHCXR8 data.\n",
        "Please download the training set and the validation set from the BMI707 Kaggle contest website (https://www.kaggle.com/c/2019bmi707-assignment-2-q5/data ). Please note that this dataset is different from the one we used in Question 3 and 4. Please DO NOT use any additional dataset (including those from NIH CXR8) to train or augment your models. Feel free to use any (ImageNet or any custom) architecture to classify all available classes. In your model with the lowest validation loss, what are the hyperparameters? What is the validation loss/accuracy? What is the training loss/accuracy? Please participate in the BMI707 internal Kaggle contest (https://www.kaggle.com/c/2019bmi707-assignment-2-q5) and compare your results with others there. An ensemble of models is allowed. The top 5 submissions with the highest accuracy on the private test set (testPrivate.tar) will receive bonus points. Please include your code in the assignment submission."
      ]
    },
    {
      "metadata": {
        "id": "ek29zDAomkTO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### The below code was submitted on kaggle and returned the second highest score on the scoreboard (as of May 1st, 2 PM). The code does not run in the Google Colab notebook."
      ]
    },
    {
      "metadata": {
        "id": "Oxe5d0k0bQBb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# navigate the repo\n",
        "import os\n",
        "print(os.listdir(\"../input\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TFvVk30iVyXL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# I copy this chunk from the primer to load the data into the kernel\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# read in the labels\n",
        "train_labels = pd.read_csv('../input/train.csv',  header=None, index_col=0)\n",
        "\n",
        "# I resize the images to make them smaller\n",
        "image_size = (224,224)\n",
        "\n",
        "# read in the training images\n",
        "train_images = []\n",
        "train_dir = '../input/train/train/'\n",
        "train_files = os.listdir(train_dir)\n",
        "for f in train_files:\n",
        "  img = Image.open(train_dir + f)\n",
        "  img = img.resize(image_size)\n",
        "  img_arr = np.array(img)\n",
        "  train_images.append(img_arr)\n",
        "\n",
        "train_X = np.array(train_images)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6-toAWgKbLf6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# read in the test images\n",
        "test_p_images = []\n",
        "test_pri_dir = '../input/testprivate/test/'\n",
        "test_pub_dir = '../input/testpublic/val/'\n",
        "test_pri_files = os.listdir(test_pri_dir)\n",
        "test_pub_files = os.listdir(test_pub_dir)\n",
        "\n",
        "# load data\n",
        "for f in test_pri_files:\n",
        "  img = Image.open(test_pri_dir + f)\n",
        "  img = img.resize(image_size)\n",
        "  img_arr = np.array(img)\n",
        "  test_p_images.append(img_arr)\n",
        "for f in test_pub_files:\n",
        "  img = Image.open(test_pub_dir + f)\n",
        "  img = img.resize(image_size)\n",
        "  img_arr = np.array(img)\n",
        "  test_p_images.append(img_arr)\n",
        "\n",
        "test_p_X = np.array(test_p_images)\n",
        "\n",
        "# create a common index \n",
        "test_p_files = test_pri_files + test_pub_files\n",
        "\n",
        "# reorder the labels so that they line up with the order of the image files\n",
        "train_labels = train_labels.reindex(train_files)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_TB7UCoAmA3A",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# I print an example image\n",
        "import matplotlib.pyplot as plt\n",
        "plt.imshow(train_images[0], cmap='gray')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "V2j6gLcfmCr_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# I binarize my data\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "label_transformer = LabelBinarizer()\n",
        "train_y = label_transformer.fit_transform(train_labels)\n",
        "\n",
        "# I display the first row\n",
        "train_y[1,:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nUf4sUSlmFqS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Finally, I add a dimension for the channel\n",
        "train_X = np.repeat(train_X[:, :, :, np.newaxis], 3, axis=3)\n",
        "test_p_X = np.repeat(test_p_X[:, :, :, np.newaxis], 3, axis=3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rNs_-KeLmHaA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# I also scale my images. This is their properties before pre-processing\n",
        "train_X.max()\n",
        "train_X.min()\n",
        "train_X.mean()\n",
        "\n",
        "train_X.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PaMq6tSQmJGW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test_p_X.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bngJZYA_mK-i",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# I define my data generators for training and testing\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "        rescale=1./255,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        samplewise_center=True, \n",
        "        samplewise_std_normalization=True,\n",
        "        horizontal_flip=False) # The heart is always on the left side\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1./255,\n",
        "                                  samplewise_center=True, \n",
        "                                  samplewise_std_normalization=True)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_cItOICzmM5W",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# compute mean and sd in case of featurewise scaling. In this case I scaled every image independently\n",
        "#train_datagen.fit(train_X)\n",
        "#test_datagen.fit(test_p_X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "H5j-FJ_hmOXi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# define model\n",
        "from keras.applications.nasnet import NASNetMobile\n",
        "from keras.preprocessing import image\n",
        "from keras.models import Model, Sequential\n",
        "from keras.layers import Dense, GlobalAveragePooling2D, Conv2D, Flatten, Dropout, MaxPooling2D\n",
        "\n",
        "base_model = NASNetMobile(weights='imagenet', include_top=False)\n",
        "# get the output of the base model\n",
        "x = base_model.output\n",
        "# I add a flatten operation \n",
        "x = GlobalAveragePooling2D()(x)\n",
        "# add a layer for binary classification. As above, I use a sigmoid activation function\n",
        "predictions = Dense(15, activation='sigmoid')(x)\n",
        "\n",
        "\n",
        "model_nasnet_imagenet = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# I compile the model and start training using the previously defined generators\n",
        "# I keep the optimizer and loss function constant for consistency\n",
        "model_nasnet_imagenet.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ou9zVB8JmQDI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# setting variables\n",
        "batch_size = 16\n",
        "\n",
        "# I train the network\n",
        "model_nasnet_imagenet.fit_generator(train_datagen.flow(train_X, train_y, batch_size=batch_size),\n",
        "                    epochs=10, \n",
        "                    steps_per_epoch=5483 // batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9f0LVBWRmRmm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "prediction = model_nasnet_imagenet.predict_generator(test_datagen.flow(test_p_X, batch_size = 1), steps = 3654)\n",
        "\n",
        "df = pd.DataFrame(test_p_files, columns=['Id'])\n",
        "df['Category'] = np.argmax(prediction, axis=1)\n",
        "#df.head()\n",
        "\n",
        "df.shape\n",
        "\n",
        "df.to_csv('submission.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MpVkb-gDV0Fj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Question 6: Limitations of the NIH CXR8 dataset and the road ahead (5 points)"
      ]
    },
    {
      "metadata": {
        "id": "5jfRAuFuV0nH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Please list three limitations of models trained from this dataset. Word limit: 150 words."
      ]
    },
    {
      "metadata": {
        "id": "EGGjMU3aU_7H",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "__Your Answer:__\n",
        "\n",
        "\n",
        "1.   Models trained on this dataset are optimized for the interpretation of X-rays that were collected in only one US hospital PACS system. Thus, the generalizability of models trained on this dataset might be limited.\n",
        "1.   The labels in the dataset were generated using natural language processing and were not evalauted by a set of independent radiologists. Thus the labels could be erroneuos.\n",
        "1.   Most images contain procedure-related artefacts, such as a label with the mode of acquisition or an indicator of X-ray orientation. This can lead to information diffusion, in which non-morphological data is linked to a diagnosis. \n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "fqK0h9zvWibE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Part 2: Recurrent Neural Networks (20 points)\n",
        "\n",
        "During the in-class exercise of Lecture 4, we implemented a simple recurrent neural network for classifying newswires from Reuters. In this question, you will implement a recurrent neural network model using long short-term memory (LSTM) or gated recurrent unit (GRU), and compare its performance with that of the simple recurrent neural network in the in-class exercise using the same dataset. "
      ]
    },
    {
      "metadata": {
        "id": "i1GWJFuVWbfO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Question 7: Build a recurrent neural network with LSTM or GRU for text classification (15 points)\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "vIuLjOHtAwgs",
        "colab_type": "code",
        "outputId": "934409a9-bc60-4b21-c8b4-88a1b77a5ce7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290
        }
      },
      "cell_type": "code",
      "source": [
        "# I have to downgrade numpy to load the dataset \n",
        "!pip install numpy==1.16.2\n",
        "import numpy as np\n",
        "print(np.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting numpy==1.16.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/35/d5/4f8410ac303e690144f0a0603c4b8fd3b986feb2749c435f7cdbb288f17e/numpy-1.16.2-cp36-cp36m-manylinux1_x86_64.whl (17.3MB)\n",
            "\u001b[K    100% || 17.3MB 2.9MB/s \n",
            "\u001b[31mdatascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31malbumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.8 which is incompatible.\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Found existing installation: numpy 1.16.3\n",
            "    Uninstalling numpy-1.16.3:\n",
            "      Successfully uninstalled numpy-1.16.3\n",
            "Successfully installed numpy-1.16.2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "1.16.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "t9mWMqrQ_mUg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# For fairness, I download the same dataset \n",
        "\n",
        "# load the required packages\n",
        "import keras\n",
        "from keras.layers import SimpleRNN, Dense, Embedding, GRU, Dropout\n",
        "from keras.models import Sequential\n",
        "from keras.datasets import reuters\n",
        "from keras.preprocessing import sequence\n",
        "\n",
        "max_features = 10000  # number of words to consider as features\n",
        "maxlen = 500          # cut texts after this number of words\n",
        "batch_size = 64\n",
        "\n",
        "# load data from keras.datasets\n",
        "(x_train_r, y_train_r), (x_test_r, y_test_r) = reuters.load_data(num_words=max_features)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EfYxSWOumpki",
        "colab_type": "code",
        "outputId": "3da91ce6-58c5-4584-e195-5f116a7eff00",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "cell_type": "code",
      "source": [
        "## Your code goes here\n",
        "################################################################################\n",
        "# I transform my data by padding, trimming and one-hot encoding\n",
        "from keras.utils import np_utils\n",
        "\n",
        "y_train_r = np_utils.to_categorical(y_train_r, 46)\n",
        "y_test_r = np_utils.to_categorical(y_test_r, 46)\n",
        "\n",
        "x_train_r = sequence.pad_sequences(x_train_r, maxlen=maxlen)\n",
        "x_test_r = sequence.pad_sequences(x_test_r, maxlen=maxlen)\n",
        "print('After padding:')\n",
        "print('The shape of x_train:', x_train_r.shape)\n",
        "print('The shape of x_test:', x_test_r.shape)\n",
        "print('The shape of y_train:', y_train_r.shape)\n",
        "print('The shape of y_test:', y_test_r.shape)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "After padding:\n",
            "The shape of x_train: (8982, 500)\n",
            "The shape of x_test: (2246, 500)\n",
            "The shape of y_train: (8982, 46)\n",
            "The shape of y_test: (2246, 46)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NiAuoUxBFOKJ",
        "colab_type": "code",
        "outputId": "b3e8b9fd-b5ca-4ae3-a3b1-14dbbb905067",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        }
      },
      "cell_type": "code",
      "source": [
        "n_epochs=3\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_features, 64, input_length=maxlen))\n",
        "model.add(GRU(64, return_sequences=True))\n",
        "model.add(GRU(64, return_sequences=True))\n",
        "model.add(GRU(64, return_sequences=True))\n",
        "model.add(GRU(64))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(46, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(x_train_r, y_train_r,\n",
        "                    epochs=n_epochs,\n",
        "                    batch_size=128,\n",
        "                    validation_split=0.3)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Train on 6287 samples, validate on 2695 samples\n",
            "Epoch 1/3\n",
            "6287/6287 [==============================] - 136s 22ms/step - loss: 2.7656 - acc: 0.3356 - val_loss: 2.1464 - val_acc: 0.3570\n",
            "Epoch 2/3\n",
            "6287/6287 [==============================] - 130s 21ms/step - loss: 2.0128 - acc: 0.4333 - val_loss: 1.8777 - val_acc: 0.4950\n",
            "Epoch 3/3\n",
            "6287/6287 [==============================] - 132s 21ms/step - loss: 1.7565 - acc: 0.5179 - val_loss: 1.7867 - val_acc: 0.5369\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1t5ATcUODF3U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "6c3a99ea-bad6-4d3f-8f81-621d57f2e294"
      },
      "cell_type": "code",
      "source": [
        "# I collect the performance values similar to the in-class excercise\n",
        "score = model.evaluate(x_test_r, y_test_r, batch_size=batch_size, verbose=1)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2246/2246 [==============================] - 31s 14ms/step\n",
            "Test loss: 1.817584462390876\n",
            "Test accuracy: 0.5298308103294747\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2NMeBxJtnxH0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Question 8: How does the performance of your model compare with that of the simple recurrent neural network model in the in-class exercise? Why? (5 points)\n",
        "\n",
        "Word limit: 100 words."
      ]
    },
    {
      "metadata": {
        "id": "mmNA0FTQky06",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "__Your Answer:__\n",
        "\n",
        "Compared to the simple RNN architecture in the in-class excercise, the model had a higher accuracy (0.42 vs. 0.53) on the test dataset and the same evaluation method. A possible reason for this could be that the cell state can be forgotten in GRU-, compared to SimpleRNN cells. Still, even the GRU-powered network performed worse than the CNN from our in-class excercise. "
      ]
    }
  ]
}